import numpy as np
import pandas as pd
from sklearn import preprocessing


def ndarray_to_df():
    # 10 training data and each one has three dimension
    # width is 2, length is 2,  height is 3
    arr = np.ndarray(shape=(10, 2, 2, 3))
    print(arr)

    # flatten arr and generate data frame.
    df = pd.DataFrame(arr.reshape(arr.shape[0], np.prod(arr.shape[1:])))
    print(df)
    return df


def df_to_ndarray():
    df = pd.DataFrame(data=np.random.random((10, 8)))
    print(df)
    # use as_matrix method or its values transform to ndarray
    print(df.as_matrix().reshape((10, 2, 4)))
    return df.values.reshape((10, 2, 4))


# after data normalization. the mean become zero and variance is 1
# data not all fall into [-1, 1] scope.
def normalize_data():
    arr = np.random.random(size=20).reshape((5, 2, 2))
    print(arr)
    arr -= np.mean(arr, axis=0)
    print("++++++++++++After subtract mean++++++++++++++++++++")
    print(arr)
    arr /= np.std(arr, axis=0)
    print("++++++++++++After divide std++++++++++++++++++++")
    print(arr)

    print("++++++++++++Mean and Variance after normalization.++++++++++==========")
    print(np.mean(arr, axis=0))
    print(np.var(arr, axis=0))

    return arr


# call normalize method from sci learn.
def sci_normalize():
    x = [[1., -1., 2.], [2., 0., 0.], [0., 1., -1111.]]
    xn = preprocessing.normalize(x, norm='l2')
    print(xn)


def get_covariance():
    # four data sample, and each has seven features.
    arr = np.random.random(size=28).reshape((4, 7))
    arr -= np.mean(arr, axis=0)
    # should divide data number - 1.
    cov = np.dot(arr.T, arr) / 3
    print(cov)
    print("++++++++++++++++++++++++++++++++")
    print(np.cov(arr.T))


def get_svd():
    arr = np.random.random(size=28).reshape((4, 7))
    cov = np.cov(arr.T)
    # a eigen vectors and b is singular values.
    a, b, c = np.linalg.svd(cov)
    print(a)
    b.sort()
    print(b)

    # decorrlate the data, map the data to space generated by eigen vector.
    # so each feature is irrelevant.
    arr_rot = np.dot(arr, a)

    # pca , use top 100 feature and discard others.
    arr_reduced = np.dot(arr, a[:, :100])

    # whiten data. after all feature has the same variance.
    arr_white = arr_rot / np.sqrt(b + 1e-5)
    print(arr_white)


def cov(a, b):
    if len(a) != len(b):
        return

    a_mean = np.mean(a)
    b_mean = np.mean(b)
    sum = 0
    for i in range(0, len(a)):
        sum += ((a[i] - a_mean) * (b[i] - b_mean))
    return sum/(len(a)-1)


def zca_whitening(inputs):
    sigma = np.dot(inputs, inputs.T)/inputs.shape[1] #inputs是经过归一化处理的，所以这边就相当于计算协方差矩阵
    U,S,V = np.linalg.svd(sigma) #奇异分解
    epsilon = 0.1                #白化的时候，防止除数为0
    ZCAMatrix = np.dot(np.dot(U, np.diag(1.0/np.sqrt(np.diag(S) + epsilon))), U.T)                     #计算zca白化矩阵
    return np.dot(ZCAMatrix, inputs)   #白化变换


get_svd()

